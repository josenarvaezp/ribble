# S3

Amazon Simple Storage Service (AWS S3) is an scalable object storage service. Businesses with any varied usecases use S3 to store and protect their data. S3 is composed of buckets and objects. An object is a file of any type while the bucket is the entity that contains the objects. An object is identified by its bucket name and key, which is a unique identifier withing the bucket. AWS is scalable and has high availability which is achieved by replication across servers within Amazon's data centres.s

Useful things to have:
- S3 Object Lock prevents objects from bein deleted or overwritten. This allows objects to have a write-once-read-many policy. 
- Using event notifications, S3 integrates with AWS services such as SNS, Lambda, and SQS. This allows programmers to create natual workflows and react to events happening to S3 objects as they happen. 
- S3 offers programmatic access by using the S3 REST API, an HTTPC interface to S3.
- S3 offers a pay-as-you go model, which means users do not need to plan ahead and purchase a predetermined amount. 

Consistency:
- S3 provides strong read-after-write, consistency for PUT and DELETE requests. This means that a read requests performed after a succesful write request will get the new data.
- Locking is a mechanism S3 offers to prevents objects being overwritten or deleted. When two concurrent writters try to lock the object, the request with the latest timestamp wins.

Multi-part uploads:
- AWS recomends that objects larger than 100 MB are uploaded using multi-part uploads. This improves the throughput as it uploads an object in parallel. It can recover from netwrok issued quicker. It also allows the users to upload a file as it is being created. This means that a 5 TB file doens't need to be stored in memory. 

- 3,500 PUT/COPY/POST/DELETE or 5,500 GET/HEAD requests per second per prefix (where prefix is a directory). Since there are not restrictions on the number of prefixes you can have, S3 can massively scale. 

Restrictions: TODO: mention this somewhere else
- S3 can only have 100 buckets. By design our framework needs a bucket which works as the workspace. 
- An object can be up to 5TB which allows this framework to handle big amounts of data
- Objects bigger than 100 MB should use multi-part upload


# Maybe features that we could use
- Creating an S3 access point means that we can use a lambda function to read data, transform it and send it back after processing. https://docs.aws.amazon.com/AmazonS3/latest/userguide/tutorial-s3-object-lambda-uppercase.html


