# SERVERLESS DATA PROCESSING

Paralellizing data processing has been in the roadmap of engineers trying to create more efficient processing frameworks for a long time. Allowing paralelization into frameworks requires a set of computers than can be used to process the data, it is clear to see that the management and configuration of this resources becomes a major bottleneck. Cloud providers have created solutions for this problem by introducing managed clusters that allow users to run big data applications without the need of having on-premise clusters. An example of this is Amazon EMR, a service that allows you to configure elastic clusters running on EC2 instances in a matter of minutes. However, recent research [2] has proven that serverless data processing frameworks outperform frameworks running on managed clusters. For this reason, in this section we eveluate different existing solutions for serverless data processing in the cloud. 

 
1. AWS serveless mapreduce

## Corral

Corral is a mapreduce inspired framework designed to run in AWS Lambda. Corral offers easy to use interfaces that developers can use to sepecify how data is mapped and reduced. For its communication, it uses S3 as the mechanism to achieve stateless shuffling. 

Workflow:
1. Like hadoop's map reduce, the input data is split into similar size chunks. Each of these chunks is the input for the mappers with  a one-to-one relation. The implmenetation of the splitting algorithms tries to to put data from the same file into the same splits, however this is not a guarantee.
2. The mappers recieve the data line-by-line and perform the developer's specific map function. 
3. The mappers output is shuffled using S3. Keys are partition into n number of reducers meaning that each mapper writes up to n different S3 files. 
4. Reducers read from the S3 files generated by the mappers and start reducing the data with the developer's specified function. The output is again written to S3, where each reducer will write one output file. Because of the shuffling data, the output of each reducer is final. 

![Alt text](./images/corral_design.svg) 
[5]

Limitations:
- Corral does not allow for a logical split. Treating each file as input for each mapper is not possible. A simple job where the user wants to perform a total calculation in each file will not be possible.
- There is a local driver that needs to persist for the duration of the job which means the frameworl is not completly serverless as the local machine needs to wait until the job is completed. 
- Mappers take input by line, which restricts the framework from processing data such as images.
- Given that the output of the mappers is only shufflued and not sorted, the reducers needs to be able to store all the input in memory. Note that in contrast to this, hadoop mapreduce sorts the mapper output which means the reducers can group the data as they recieve it meaning that when aggregating the date they only need to fit into memory the number of items in each group. 

## MARLA (MApReduce on Lambda)

The MARLA architecture is composed of a coordinator, mappers and reducers and uses S3 as the storage mechanism to read input data and produce the output from the mappers and reducers. 

Workflow:
1. The execution of the frameworks begins when the input data is uploaded into the input bucket. This event invokes the coordinator. It is important to notice that this framework does not use a driver running in a separate machine, instead, the mapreduce job starts with a trigger from S3, once data is submitted into the input bucket. An advantage of not using the driver is that it allows the framwork to be triggered on demand.
2. The coordinator is responsible splitting the data into chucnk size such that the data can be split into the number of mappers the user has specified. The coordinator is splits the data according to the number of mappers the user has specified and the amount of memory each memory is allowed to use. One the coordinator has defined the optimal splits, it invokes the first lambda mapper.
3. The first mapper lambda starts up and it recursively invokes the rest of the mappers. In this process, every time a new mappers starts up, they invoke a new mapper until all the mappers have been invoked.
4. Once all mappers are up and runnning, they read their respective data chunk and process the data ccording to the user's defined mapper function. Once a mapper is done processing data, the result is sorted and split into logical chunks such that all keys that start with the same letter are stored into the same chunk. Each chunk will be reduced by a different reducer. Each chunk is sent into S3 as independent files. 
5. The first reducer is invoked after the last mapper starts. This first reducer waits until the data written to the last partition is finished and then start invoking the rest of the partitioners. Each reducer will then read as many mapped chunks as it can and will process the data using the user's defined reducer function. The reducers data is finally sent into an S3 files. 

![Alt text](./images/marla_arch.png) 
[6]

Failure handling:
- Coordinator: If the coordinator fails the frameworks fails to start and will need to be triggered again.
- Mapper: if a mapper fails, its data will not be uploaded to S3 and the reducers will not be invoked. The job will finish at this point however the data from the mappers that didn't fail will be stored in S3 and the user will need to handle the situation.
- Reducers: given that reducers are independent of each other, failed reducers will not produce any output data however, the rest of the reducers will output their reduced data to the output bucket. 

Limitations:
- Given that the shuffling of data depends on the first letter of the keys, if the keys are not well distributed alphabetiaclly but this can be solved if the mapper changes the key using a hash function.
- Failures are not handled gracefully. In hadoop, failures are re-executed however, if the mappers or reducers fail the frameworks finishes with incorrect data. There is no automatic re-execution strategy. 
- Data in each mapper needs to fit into memory as the results of the mapper is sorted before sending to the reducers

3. Cloud map reduce: A MapReduce Implementation on top of a Cloud Operating System  

4. Evaluating serverless data processing frameworks

Similitudes:
- Driver: where does the driver live and how does it communicate with the rest of the resouces. Synch communiction to keep track of resources means that the driver must persist for the duration of the job. This brings up two probles, first it means that the platfrom depends on the driver running for the duration of the job and this makes the framework not entirely serverless. In the other side, if the framework runs in a serverless function then it means the framework is restricted to the maximum amount of time the functino can run for (AWS has a 15 min max). 

- Communication: serverless functions do not have a natural way of communicating with each other, and given their stateless nature, a work around is used by all frameworks. Corral uses s3, and it creates a folder for each key it encountersso that the reducers only need to look at one forder (shuffling step done). PyWren rellies on the programmer to define the shuffling step (given that we want this framework to be as simple as possible we avoid doing that). It is important to notice that given that we depend on external services, extra care should be taken to make sure each of the resources can scale up as desired to avoid having a bottleneck anywhere in the system. Quobole relies on a single VM for inter-process comuniction which becomes a bottleneck for big datasets.


## My framework considerations
- To simplify the framework to developers, all data is combined in the mapper before sending it to the reducers. This means that the output of the mapper needs to be no larger than the memory limit of the serverless function. Note that the input data for the mapper is streamed into the mapper, hence there is no correlation between the input size with the functins's memory size. For example, a mapper that reads 1 GB, but beacause of the filtering, outputs a 1 MB size does not need to have a memory size of 1GB. 



## Resources:
1. https://aws.amazon.com/blogs/compute/ad-hoc-big-data-processing-made-simple-with-serverless-mapreduce/  
2. https://www.ise.tu-berlin.de/fileadmin/fg308/publications/2020/PrePrint_2020__WoSC_Eval_SDPF.pdf
3. https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.177.4059&rep=rep1&type=pdf 
4. https://github.com/bcongdon/corral
5. https://benjamincongdon.me/blog/2018/05/02/Introducing-Corral-A-Serverless-MapReduce-Framework/
6. MARLA Vicent Giménez-Alventosa, Germán Moltó, and Miguel Caballer. 2019.
A framework and a performance assessment for serverless MapReduce
on AWS Lambda. Future Generation Computer Systems 97 (2019), 
- https://aws.amazon.com/emr/features/?nc=sn&loc=2&dn=1 AMAZON EMR Features