# SERVERLESS DATA PROCESSING

Paralellizing data processing has been in the roadmap of engineers trying to create more efficient processing frameworks for a long time. Allowing paralelization into frameworks requires a set of computers than can be used to process the data, it is clear to see that the management and configuration of this resources becomes a major bottleneck. Cloud providers have created solutions for this problem by introducing managed clusters that allow users to run big data applications without the need of having on-premise clusters. An example of this is Amazon EMR, a service that allows you to configure elastic clusters running on EC2 instances in a matter of minutes. However, recent research [2] has proven that serverless data processing frameworks outperform frameworks running on managed clusters. For this reason, in this section we eveluate different existing solutions for serverless data processing in the cloud. 

 
1. AWS serveless mapreduce

2. Corral

Corral is a mapreduce inspired framework designed to run in AWS Lambda. Corral offers easy to use interfaces that developers can use to sepecify how data is mapped and reduced. For its communication, it uses S3 as the mechanism to achieve stateless shuffling. 

Workflow:
1. Like hadoop's map reduce, the input data is split into similar size chunks. Each of these chunks is the input for the mappers with  a one-to-one relation. The implmenetation of the splitting algorithms tries to to put data from the same file into the same splits, however this is not a guarantee.
2. The mappers recieve the data line-by-line and perform the developer's specific map function. 
3. The mappers output is shuffled using S3. Keys are partition into n number of reducers meaning that each mapper writes up to n different S3 files. 
4. Reducers read from the S3 files generated by the mappers and start reducing the data with the developer's specified function. The output is again written to S3, where each reducer will write one output file. Because of the shuffling data, the output of each reducer is final. 

![Alt text](./images/corral_design.svg)
<img src="./images/corral_design.svg">

Limitations:
- Corral does not allow for a logical split. Treating each file as input for each mapper is not possible. A simple job where the user wants to perform a total calculation in each file will not be possible.
- There is a local driver that needs to persist for the duration of the job which means the frameworl is not completly serverless as the local machine needs to wait until the job is completed. 
- Mappers take input by line, which restricts the framework from processing data such as images.
- Given that the output of the mappers is only shufflued and not sorted, the reducers needs to be able to store all the input in memory. Note that in contrast to this, hadoop mapreduce sorts the mapper output which means the reducers can group the data as they recieve it meaning that when aggregating the date they only need to fit into memory the number of items in each group. 


3. Cloud map reduce: A MapReduce Implementation on top of a Cloud Operating System  

4. Evaluating serverless data processing frameworks

Similitudes:
- Driver: where does the driver live and how does it communicate with the rest of the resouces. Synch communiction to keep track of resources means that the driver must persist for the duration of the job. This brings up two probles, first it means that the platfrom depends on the driver running for the duration of the job and this makes the framework not entirely serverless. In the other side, if the framework runs in a serverless function then it means the framework is restricted to the maximum amount of time the functino can run for (AWS has a 15 min max). 

- Communication: serverless functions do not have a natural way of communicating with each other, and given their stateless nature, a work around is used by all frameworks. Corral uses s3, and it creates a folder for each key it encountersso that the reducers only need to look at one forder (shuffling step done). PyWren rellies on the programmer to define the shuffling step (given that we want this framework to be as simple as possible we avoid doing that). It is important to notice that given that we depend on external services, extra care should be taken to make sure each of the resources can scale up as desired to avoid having a bottleneck anywhere in the system. Quobole relies on a single VM for inter-process comuniction which becomes a bottleneck for big datasets.


## My framework considerations
- To simplify the framework to developers, all data is combined in the mapper before sending it to the reducers. This means that the output of the mapper needs to be no larger than the memory limit of the serverless function. Note that the input data for the mapper is streamed into the mapper, hence there is no correlation between the input size with the functins's memory size. For example, a mapper that reads 1 GB, but beacause of the filtering, outputs a 1 MB size does not need to have a memory size of 1GB. 



## Resources:
1. https://aws.amazon.com/blogs/compute/ad-hoc-big-data-processing-made-simple-with-serverless-mapreduce/  
2. https://www.ise.tu-berlin.de/fileadmin/fg308/publications/2020/PrePrint_2020__WoSC_Eval_SDPF.pdf
3. https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.177.4059&rep=rep1&type=pdf 
4. https://github.com/bcongdon/corral
5. https://benjamincongdon.me/blog/2018/05/02/Introducing-Corral-A-Serverless-MapReduce-Framework/

- https://aws.amazon.com/emr/features/?nc=sn&loc=2&dn=1 AMAZON EMR Features