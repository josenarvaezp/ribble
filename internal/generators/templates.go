package generators

const mapCoordinatorTemplate = `
// Code generated by ribble DO NOT EDIT.
// |\   \\\\__     o
// | \_/    o \    o 
// > _   (( <_  oo  
// | / \__+___/      
// |/     |/

package main

import (
	"context"
	"fmt"

	"github.com/aws/aws-lambda-go/lambda"
	log "github.com/sirupsen/logrus"

	"github.com/josenarvaezp/displ/pkg/lambdas"
)

var c *lambdas.Coordinator

func init() {
	// set logger
	log.SetLevel(log.ErrorLevel)

	var err error
	c, err = lambdas.NewCoordinator({{.Local}})
	if err != nil {
		log.WithError(err).Fatal("Error starting coordinator")
		return
	}
}

func HandleRequest(ctx context.Context, request lambdas.CoordinatorInput) error {
	// update coordinator
	c.UpdateCoordinatorWithRequest(ctx, request)

	// set coordinator logger
	coordinatorLogger := log.WithFields(log.Fields{
		"Job ID": c.JobID.String(),
	})

	// log init
	nextLogToken, _ := c.LogEvents(
		ctx,
		[]string{
			"Coordinator starting...",
			fmt.Sprintf("Waiting for %d mappers...", request.NumMappers),
		},
		nil, // empty token as it is the first log
	)

	// start mappers
	err := c.StartMappers(ctx, request.NumQueues, request.FunctionName)
	if err != nil {
		coordinatorLogger.WithError(err).Error("Error starting the mappers")
		return err
	}

	// waits until mappers are done
	nextLogToken, err = c.AreMappersDone(ctx, nextLogToken)
	if err != nil {
		coordinatorLogger.WithError(err).Error("Error reading mappers done queue")
		return err
	}

	// log mappers done
	nextLogToken, _ = c.LogEvents(
		ctx,
		[]string{
			"Mappers execution completed...",
			fmt.Sprintf("Waiting for %d reducers...", request.NumQueues),
		},
		nextLogToken,
	)

	// invoke reducers
	if err := c.InvokeReducers(ctx, "{{.LambdaAggregator}}"); err != nil {
		coordinatorLogger.WithError(err).Error("Error invoking reducers")
		return nil
	}

	// wait until reducers are done
	nextLogToken, err = c.AreReducersDone(ctx, nextLogToken)
	if err != nil {
		coordinatorLogger.WithError(err).Error("Error reading reducers done queue")
		return err
	}

	// log reducers done
	nextLogToken, _ = c.LogEvents(
		ctx,
		[]string{
			"Reducers execution completed...",
			fmt.Sprintf(
				"Job completed successfully, output is available at the S3 bucket %s...",
				c.JobID.String(),
			),
		},
		nextLogToken,
	)

	// indicate reducers are done
	if err := c.WriteDoneObject(ctx); err != nil {
		coordinatorLogger.WithError(err).Error("Error writing done signal")
		return err
	}

	return nil
}

func main() {
	lambda.Start(HandleRequest)
}

`

const randomCoordinatorTemplate = `
// Code generated by ribble DO NOT EDIT.
// |\   \\\\__     o
// | \_/    o \    o
// > _   (( <_  oo
// | / \__+___/
// |/     |/

package main

import (
	"context"
	"fmt"
	"time"

	"github.com/aws/aws-sdk-go-v2/aws"
	"github.com/aws/aws-sdk-go-v2/service/s3"
	"github.com/aws/aws-lambda-go/lambda"
	log "github.com/sirupsen/logrus"

	"github.com/josenarvaezp/displ/pkg/lambdas"
)

var c *lambdas.Coordinator

func init() {
	// set logger
	log.SetLevel(log.ErrorLevel)

	var err error
	c, err = lambdas.NewCoordinator({{.Local}})
	if err != nil {
		log.WithError(err).Fatal("Error starting coordinator")
		return
	}
}

func HandleRequest(ctx context.Context, request lambdas.CoordinatorInput) error {
	// update coordinator
	c.UpdateCoordinatorWithRequest(ctx, request)

	// set coordinator logger
	coordinatorLogger := log.WithFields(log.Fields{
		"Job ID": c.JobID.String(),
	})

	// log init
	nextLogToken, _ := c.LogEvents(
		ctx,
		[]string{
			"Coordinator starting...",
			fmt.Sprintf("Waiting for %d mappers...", request.NumMappers),
		},
		nil, // empty token as it is the first log
	)

	// start mappers
	err := c.StartMappers(ctx, request.NumQueues, request.FunctionName)
	if err != nil {
		coordinatorLogger.WithError(err).Error("Error starting the mappers")
		return err
	}

	// waits until mappers are done
	nextLogToken, err = c.AreMappersDone(ctx, nextLogToken)
	if err != nil {
		coordinatorLogger.WithError(err).Error("Error reading mappers done queue")
		return err
	}

	// log mappers done
	nextLogToken, _ = c.LogEvents(
		ctx,
		[]string{
			"Mappers execution completed...",
			fmt.Sprintf("Waiting for %d reducers...", request.NumQueues),
		},
		nextLogToken,
	)

	// invoke reducers
	if err := c.InvokeReducers(ctx, "{{.LambdaAggregator}}"); err != nil {
		coordinatorLogger.WithError(err).Error("Error invoking reducers")
		return nil
	}

	// wait until reducers are done
	nextLogToken, err = c.AreReducersDone(ctx, nextLogToken)
	if err != nil {
		coordinatorLogger.WithError(err).Error("Error reading reducers done queue")
		return err
	}

	// log reducers done
	nextLogToken, _ = c.LogEvents(
		ctx,
		[]string{
			"Reducers execution completed...",
			"Waiting for final reducer...",
		},
		nextLogToken,
	)

	// invoke final reducer
	if err := c.InvokeReducer(ctx, "{{.LambdaFinalAggregator}}"); err != nil {
		coordinatorLogger.WithError(err).Error("Error invoking final reducer")
		return nil
	}

	// wait until job is done
	jobBucket := c.JobID.String()
	key := "output"
	bytes := fmt.Sprintf("bytes=%d-%d", 0, 1)
	for true {
		_, err := c.ObjectStoreAPI.GetObject(ctx, &s3.GetObjectInput{
			Bucket: &jobBucket,
			Key:    &key,
			Range:  aws.String(bytes),
		})
		if err != nil {
			// sleep for 5 seconds before trying to get the object
			time.Sleep(5 * time.Second)
			break
		}

		// job done
	}

	// log job done
	nextLogToken, _ = c.LogEvents(
		ctx,
		[]string{
			"Final reducer execution completed...",
			fmt.Sprintf(
				"Job completed successfully, output is available at the S3 bucket %s...",
				c.JobID.String(),
			),
		},
		nextLogToken,
	)

	return nil
}

func main() {
	lambda.Start(HandleRequest)
}

`

var reducerTemplate = `
FROM golang:1.16 as build

ARG CGO_ENABLED=0

# create work directory
WORKDIR /build

# install tools
RUN apt-get update && apt-get install -y upx

# add dependancies
ADD go.mod go.sum ./
RUN go mod download

# add source files
ADD ./pkg ./pkg
ADD ./internal ./internal
ADD ./build/lambda_gen/{{.JobID}} ./build/lambda_gen/{{.JobID}}
ADD ./{{.Workspace}} ./{{.Workspace}}

# build aggregators
RUN env GOOS=linux GOARCH=amd64 go build -ldflags "-s -w" -o /build/map_aggregator ./build/lambda_gen/{{.JobID}}/map_aggregator/map_aggregator.go

# compress
RUN upx --best --lzma /build/map_aggregator

# Build runtime for map_aggregator_{{.JobID}}
FROM alpine as map_aggregator

COPY --from=build /build/map_aggregator /map_aggregator

ENTRYPOINT [ "/map_aggregator" ]

`

var randomReducerTemplate = `
FROM golang:1.16 as build

ARG CGO_ENABLED=0

# create work directory
WORKDIR /build

# install tools
RUN apt-get update && apt-get install -y upx

# add dependancies
ADD go.mod go.sum ./
RUN go mod download

# add source files
ADD ./pkg ./pkg
ADD ./internal ./internal
ADD ./build/lambda_gen/{{.JobID}} ./build/lambda_gen/{{.JobID}}
ADD ./{{.Workspace}} ./{{.Workspace}}

# build aggregators
RUN env GOOS=linux GOARCH=amd64 go build -ldflags "-s -w" -o /build/map_random_aggregator ./build/lambda_gen/{{.JobID}}/map_random_aggregator/map_random_aggregator.go
RUN env GOOS=linux GOARCH=amd64 go build -ldflags "-s -w" -o /build/final_aggregator ./build/lambda_gen/{{.JobID}}/final_aggregator/final_aggregator.go 

# compress
RUN upx --best --lzma /build/map_random_aggregator
RUN upx --best --lzma /build/final_aggregator

# Build runtime for map_random_aggregator aggregator
FROM alpine as map_random_aggregator

COPY --from=build /build/map_random_aggregator /map_random_aggregator

ENTRYPOINT [ "/map_random_aggregator" ]

# Build runtime for final_aggregator aggregator
FROM alpine as final_aggregator

COPY --from=build /build/final_aggregator /final_aggregator

ENTRYPOINT [ "/final_aggregator" ]

`

const dockerfileTemplate = `
# Code generated by ribble DO NOT EDIT.
# |\   \\\\__     o
# | \_/    o \    o 
# > _   (( <_  oo  
# | / \__+___/      
# |/     |/

FROM golang as build

ARG CGO_ENABLED=0

# create work directory
WORKDIR /build

# install tools
RUN apt-get update && apt-get install -y upx

# add dependancies
ADD go.mod go.sum ./
RUN go mod download

# add source files
ADD ./pkg ./pkg
ADD ./internal ./internal
ADD ./build/lambda_gen/{{.JobID}} ./build/lambda_gen/{{.JobID}}
ADD ./{{.Workspace}} ./{{.Workspace}}

# build lambdas
RUN env GOOS=linux GOARCH=amd64 go build -ldflags "-s -w" -o /build/lambdas/ ./build/lambda_gen/{{.JobID}}/{{.FunctionType}}/{{.FunctionName}}.go

# compress
RUN upx --best --lzma /build/lambdas/{{.FunctionName}}

# Build runtime for {{.FunctionType}}_{{.JobID}}
FROM alpine as {{.FunctionType}}

COPY --from=build /build/lambdas/{{.FunctionName}} /lambdas/{{.FunctionName}}

ENTRYPOINT [ "/lambdas/{{.FunctionName}}" ]
`

const mapTemplate = `
// Code generated by ribble DO NOT EDIT.
// |\   \\\\__     o
// | \_/    o \    o
// > _   (( <_  oo
// | / \__+___/
// |/     |/

package main

import (
	"context"
	"os"
	
	"github.com/aws/aws-lambda-go/lambda"
	log "github.com/sirupsen/logrus"

	"github.com/josenarvaezp/displ/pkg/lambdas"
	"{{.PackagePath}}"
)

var m *lambdas.Mapper

func init() {
	// set logger
	log.SetLevel(log.ErrorLevel)

	var err error
	m, err = lambdas.NewMapper({{.Local}})
	if err != nil {
		log.WithError(err).Fatal("Error starting mapper")
		return
	}
}

func HandleRequest(ctx context.Context, request lambdas.MapperInput) error {
	// update mapper
	m.UpdateMapperWithRequest(ctx, request)

	// set mapper logger
	mapperLogger := log.WithFields(log.Fields{
		"Job ID": m.JobID.String(),
		"Map ID": m.MapID.String(),
	})

	// keep a dictionary with the number of batches per queue
	batchMetadata := make(map[int]int64)

	for _, object := range request.Mapping.Objects {
		// download file
		filename, err := m.DownloadFile(object)
		if err != nil {
			mapperLogger.
				WithFields(log.Fields{
					"Bucket": object.Bucket,
					"Object": object.Key,
				}).
				WithError(err).
				Error("Error downloading file")
			return err
		}

		// user function starts here
		mapOutput := lambdas.RunMapAggregator(*filename, {{.PackageName}}.{{.Function}})

		// send output to reducers via queues
		err = m.EmitMap(ctx, mapOutput, batchMetadata)
		if err != nil {
			mapperLogger.
				WithFields(log.Fields{
					"Bucket": object.Bucket,
					"Object": object.Key,
				}).
				WithError(err).
				Error("Error sending map output to reducers")
			return err
		}

		// clean up file in /tmp
		err = os.Remove(*filename)
		if err != nil {
			mapperLogger.
				WithFields(log.Fields{
					"Bucket": object.Bucket,
					"Object": object.Key,
				}).
				WithError(err).
				Error("Error cleaning file from /temp")
			return err
		}
	}

	// send batch metadata to sqs
	if err := m.SendBatchMetadata(ctx, batchMetadata); err != nil {
		mapperLogger.WithError(err).Error("Error sending metadata to streams")
		return err
	}

	// send event to queue indicating this mapper has completed
	if err := m.SendFinishedEvent(ctx); err != nil {
		mapperLogger.WithError(err).Error("Error sending done event to stream")
		return err
	}

	return nil
}

func main() {
	lambda.Start(HandleRequest)
}

`

const mapRandomTemplate = `
// Code generated by ribble DO NOT EDIT.
// |\   \\\\__     o
// | \_/    o \    o
// > _   (( <_  oo
// | / \__+___/
// |/     |/

package main

import (
	"context"
	"os"

	"github.com/aws/aws-lambda-go/lambda"
	log "github.com/sirupsen/logrus"

	"github.com/josenarvaezp/displ/pkg/lambdas"
	"{{.PackagePath}}"
)

var m *lambdas.Mapper

func init() {
	// set logger
	log.SetLevel(log.ErrorLevel)

	var err error
	m, err = lambdas.NewMapper({{.Local}})
	if err != nil {
		log.WithError(err).Fatal("Error starting mapper")
		return
	}
}

func HandleRequest(ctx context.Context, request lambdas.MapperInput) error {
	// update mapper
	m.UpdateMapperWithRequest(ctx, request)

	// set mapper logger
	mapperLogger := log.WithFields(log.Fields{
		"Job ID": m.JobID.String(),
		"Map ID": m.MapID.String(),
	})

	// keep a dictionary with the number of messages per queue
	messageMetadata := make(map[int]int64)

	// create random number generator with seed
	randGen := m.InitRandomSeed()

	for _, object := range request.Mapping.Objects {
		// download file
		filename, err := m.DownloadFile(object)
		if err != nil {
			mapperLogger.
				WithFields(log.Fields{
					"Bucket": object.Bucket,
					"Object": object.Key,
				}).
				WithError(err).
				Error("Error downloading file")
			return err
		}

		// user function starts here
		mapOutput := lambdas.RunMapAggregator(*filename, {{.PackageName}}.{{.Function}})

		// send output to reducers via queues
		err = m.EmitRandom(ctx, mapOutput, messageMetadata, randGen)
		if err != nil {
			mapperLogger.
				WithFields(log.Fields{
					"Bucket": object.Bucket,
					"Object": object.Key,
				}).
				WithError(err).
				Error("Error sending map output to reducers")
			return err
		}

		// clean up file in /tmp
		err = os.Remove(*filename)
		if err != nil {
			mapperLogger.
				WithFields(log.Fields{
					"Bucket": object.Bucket,
					"Object": object.Key,
				}).
				WithError(err).
				Error("Error cleaning file from /temp")
			return err
		}
	}

	// send message metadata to sqs
	if err := m.SendBatchMetadata(ctx, messageMetadata); err != nil {
		mapperLogger.WithError(err).Error("Error sending metadata to streams")
		return err
	}

	// send event to queue indicating this mapper has completed
	if err := m.SendFinishedEvent(ctx); err != nil {
		mapperLogger.WithError(err).Error("Error sending done event to stream")
		return err
	}

	return nil
}

func main() {
	lambda.Start(HandleRequest)
}

`

const reduceRandomMapAggregatorTemplate = `
// Code generated by ribble DO NOT EDIT.
// |\   \\\\__     o
// | \_/    o \    o
// > _   (( <_  oo
// | / \__+___/
// |/     |/

package main

import (
	"context"
	"encoding/json"
	"fmt"
	"sync"

	"github.com/aws/aws-lambda-go/lambda"
	"github.com/aws/aws-sdk-go-v2/service/sqs"
	sqsTypes "github.com/aws/aws-sdk-go-v2/service/sqs/types"
	log "github.com/sirupsen/logrus"

	"github.com/josenarvaezp/displ/pkg/lambdas"
	"github.com/josenarvaezp/displ/pkg/aggregators"
)

var r *lambdas.Reducer

func init() {
	// set logger
	log.SetLevel(log.ErrorLevel)

	var err error
	r, err = lambdas.NewRandomReducer({{.Local}})
	if err != nil {
		log.WithError(err).Fatal("Error starting reducer")
		return
	}
}

func HandleRequest(ctx context.Context, request lambdas.ReducerInput) error {
	// update reducer
	r.UpdateReducerWithRequest(ctx, request)

	// get reduce queue information
	queueName := fmt.Sprintf("%s-%d", r.JobID.String(), request.QueuePartition)
	queueURL := lambdas.GetQueueURL(queueName, r.Region, r.AccountID, r.Local)

	// set reducer logger
	reducerLogger := log.WithFields(log.Fields{
		"Job ID":          r.JobID.String(),
		"Reducer ID":      r.ReducerID.String(),
		"Queue Partition": r.QueuePartition,
	})

	// set wait group
	var wg sync.WaitGroup

	// get checkpoint data
	checkpointData, err := r.GetCheckpointData(ctx, &wg)
	if err != nil {
		reducerLogger.WithError(err).Error("Error reading checkpoint")
		return err
	}

	// batch metadata - number of batches the reducer needs to process
	totalMessagesToProcess, err := r.GetNumberOfBatchesToProcess(ctx)
	if err != nil {
		reducerLogger.WithError(err).Error("Error getting queue metadata")
		return err
	}
	totalProcessedMessages := 0

	// checkpoint info
	processedMessagesWithoutCheckpoint := 0
	checkpointData.LastCheckpoint++

	// holds the intermediate results
	intermediateReducedMap := make(aggregators.MapAggregator)

	// processedMessagesDeleteInfo holds the data to delete messages from queue
	processedMessagesDeleteInfo := make([]sqsTypes.DeleteMessageBatchRequestEntry, lambdas.MaxMessagesWithoutCheckpoint)

	// use same parameters for all get messages requests
	recieveMessageParams := &sqs.ReceiveMessageInput{
		QueueUrl:            &queueURL,
		MaxNumberOfMessages: lambdas.MaxItemsPerBatch,
		MessageAttributeNames: []string{
			lambdas.MessageIDAttribute,
		},
		WaitTimeSeconds: int32(5),
	}

	// recieve messages until we are done processing all queue
	for true {
		if processedMessagesWithoutCheckpoint == lambdas.MaxMessagesBeforeCheckpointComplete && checkpointData.LastCheckpoint != 1 {
			// check that the last checkpoint has completed before processing any more messages
			// we give a buffer of 15,000 new messages for saving the checkpoint which happens
			// in the background. If this point is reached it means we have processed 115,000 messages
			// without deleting from the queue which is close to the aws limit for queues
			wg.Wait()
		}

		if processedMessagesWithoutCheckpoint == lambdas.MaxMessagesWithoutCheckpoint {
			// We need to delete the messages read from the sqs queue and we create a checkpoint
			// in S3 as the fault tolerant mechanism. Saving the checkpoint can be done concurrently
			// in the background while we keep processing messages

			// merge the dedupe map so that the read dedupe map is up to date
			r.DedupeSimple.Merge()

			// save intermediate dedupe
			wg.Add(1)
			go r.SaveIntermediateDedupe(ctx, checkpointData.LastCheckpoint, r.DedupeSimple.WriteMap, &wg)

			// save intermediate map
			wg.Add(1)
			go r.SaveIntermediateOutput(ctx, intermediateReducedMap, checkpointData.LastCheckpoint, &wg)

			// update output map with reduced intermediate results
			wg.Add(1)
			go r.Output.UpdateOutput(intermediateReducedMap, &wg)

			// delete all messages from queue
			wg.Add(1)
			go r.DeleteIntermediateMessagesFromQueue(ctx, queueURL, processedMessagesDeleteInfo, &wg)

			// update checkpoint info
			checkpointData.LastCheckpoint++
			processedMessagesWithoutCheckpoint = 0
			processedMessagesDeleteInfo = make([]sqsTypes.DeleteMessageBatchRequestEntry, lambdas.MaxMessagesWithoutCheckpoint)
			intermediateReducedMap = make(aggregators.MapAggregator)
			r.DedupeSimple.WriteMap = lambdas.InitDedupeSimpleMap()
		}

		// call sqs receive messages
		output, err := r.QueuesAPI.ReceiveMessage(ctx, recieveMessageParams)
		if err != nil {
			reducerLogger.WithError(err).Error("Error reading from queue")
			return err
		}

		// process messages
		for _, message := range output.Messages {
			processedMessagesWithoutCheckpoint++

			// add delete info
			processedMessagesDeleteInfo[processedMessagesWithoutCheckpoint] = sqsTypes.DeleteMessageBatchRequestEntry{
				Id:            message.MessageId,
				ReceiptHandle: message.ReceiptHandle,
			}

			// get message attributes
			currentMessageID := *message.MessageAttributes[lambdas.MessageIDAttribute].StringValue

			// check if message has already been processed
			if !r.DedupeSimple.IsMessageProcessed(currentMessageID) {

				// process message
				// unmarshall message body
				var reduceMessage *aggregators.ReduceMessage
				body := []byte(*message.Body)
				err = json.Unmarshal(body, &reduceMessage)
				if err != nil {
					return err
				}

				// process message
				if err := intermediateReducedMap.Reduce(reduceMessage); err != nil {
					reducerLogger.WithError(err).Error("Error processing message")
					return err
				}

				// update dedupe and messages processed count
				r.DedupeSimple.UpdateMessageProcessed(currentMessageID)
				totalProcessedMessages++
			}
		}

		// check if we are done processing values
		if totalProcessedMessages == *totalMessagesToProcess {
			break
		}
	}

	// wait in case reducers is saving checkpoint in the background
	wg.Wait()

	// update output map with reduced intermediate results
	wg.Add(1)
	go r.Output.UpdateOutput(intermediateReducedMap, &wg)

	// delete all messages from queue
	wg.Add(1)
	go r.DeleteIntermediateMessagesFromQueue(ctx, queueURL, processedMessagesDeleteInfo, &wg)

	wg.Wait()

	// write reducer output
	messagesSent, err := r.EmitValuesToFinalReducer(ctx)
	if err != nil {
		reducerLogger.WithError(err).Error("Error sending reducer output to final reduce queue")
		return err
	}

	// send message metadata to sqs
	if err := r.SendMetadata(ctx, messagesSent); err != nil {
		reducerLogger.WithError(err).Error("Error sending metadata to final stream")
		return err
	}

	// indicate reducer has finished
	err = r.SendFinishedEvent(ctx)
	if err != nil {
		reducerLogger.WithError(err).Error("Error sending done message")
		return err
	}

	return nil
}

func main() {
	lambda.Start(HandleRequest)
}

`

const reduceMapAggregatorTemplate = `
// Code generated by ribble DO NOT EDIT.
// |\   \\\\__     o
// | \_/    o \    o
// > _   (( <_  oo
// | / \__+___/
// |/     |/

package main

import (
	"context"
	"encoding/json"
	"fmt"
	"strconv"
	"sync"

	"github.com/aws/aws-lambda-go/lambda"
	"github.com/aws/aws-sdk-go-v2/service/sqs"
	sqsTypes "github.com/aws/aws-sdk-go-v2/service/sqs/types"
	log "github.com/sirupsen/logrus"

	"github.com/josenarvaezp/displ/pkg/lambdas"
	"github.com/josenarvaezp/displ/pkg/aggregators"

	{{ if or .WithFilter .WithSort }} 
	"{{.PackagePath}}"
	{{ end }}
)

var r *lambdas.Reducer

func init() {
	// set logger
	log.SetLevel(log.ErrorLevel)

	var err error
	r, err = lambdas.NewMapReducer({{.Local}})
	if err != nil {
		log.WithError(err).Fatal("Error starting reducer")
		return
	}
}

func HandleRequest(ctx context.Context, request lambdas.ReducerInput) error {
	// update reducer
	r.UpdateReducerWithRequest(ctx, request)

	// get reduce queue information
	queueName := fmt.Sprintf("%s-%d", r.JobID.String(), request.QueuePartition)
	queueURL := lambdas.GetQueueURL(queueName, r.Region, r.AccountID, r.Local)

	// set reducer logger
	reducerLogger := log.WithFields(log.Fields{
		"Job ID":          r.JobID.String(),
		"Reducer ID":      r.ReducerID.String(),
		"Queue Partition": r.QueuePartition,
	})

	// set wait group
	var wg sync.WaitGroup

	// get checkpoint data
	checkpointData, err := r.GetCheckpointData(ctx, &wg)
	if err != nil {
		reducerLogger.WithError(err).Error("Error reading checkpoint")
		return err
	}

	// batch metadata - number of batches the reducer needs to process
	totalBatchesToProcess, err := r.GetNumberOfBatchesToProcess(ctx)
	if err != nil {
		reducerLogger.WithError(err).Error("Error getting queue metadata")
		return err
	}
	totalProcessedBatches := 0

	// checkpoint info
	processedMessagesWithoutCheckpoint := 0
	checkpointData.LastCheckpoint++

	// holds the intermediate results
	intermediateReducedMap := make(aggregators.MapAggregator)

	// processedMessagesDeleteInfo holds the data to delete messages from queue
	processedMessagesDeleteInfo := make([]sqsTypes.DeleteMessageBatchRequestEntry, lambdas.MaxMessagesWithoutCheckpoint)

	// use same parameters for all get messages requests
	recieveMessageParams := &sqs.ReceiveMessageInput{
		QueueUrl:            &queueURL,
		MaxNumberOfMessages: lambdas.MaxItemsPerBatch,
		MessageAttributeNames: []string{
			lambdas.MapIDAttribute,
			lambdas.BatchIDAttribute,
			lambdas.MessageIDAttribute,
		},
		WaitTimeSeconds: int32(5),
	}

	// recieve messages until we are done processing all queue
	for true {
		if processedMessagesWithoutCheckpoint == lambdas.MaxMessagesBeforeCheckpointComplete && checkpointData.LastCheckpoint != 1 {
			// check that the last checkpoint has completed before processing any more messages
			// we give a buffer of 15,000 new messages for saving the checkpoint which happens
			// in the background. If this point is reached it means we have processed 115,000 messages
			// without deleting from the queue which is close to the aws limit for queues
			wg.Wait()
		}

		if processedMessagesWithoutCheckpoint == lambdas.MaxMessagesWithoutCheckpoint {
			// We need to delete the messages read from the sqs queue and we create a checkpoint
			// in S3 as the fault tolerant mechanism. Saving the checkpoint can be done concurrently
			// in the background while we keep processing messages

			// merge the dedupe map so that the read dedupe map is up to date
			r.Dedupe.Merge()

			// save intermediate dedupe
			wg.Add(1)
			go r.SaveIntermediateDedupe(ctx, checkpointData.LastCheckpoint, r.Dedupe.WriteMap, &wg)

			// save intermediate map
			wg.Add(1)
			go r.SaveIntermediateOutput(ctx, intermediateReducedMap, checkpointData.LastCheckpoint, &wg)

			// update output map with reduced intermediate results
			wg.Add(1)
			go r.Output.UpdateOutput(intermediateReducedMap, &wg)

			// delete all messages from queue
			wg.Add(1)
			go r.DeleteIntermediateMessagesFromQueue(ctx, queueURL, processedMessagesDeleteInfo, &wg)

			// update checkpoint info
			checkpointData.LastCheckpoint++
			processedMessagesWithoutCheckpoint = 0
			processedMessagesDeleteInfo = make([]sqsTypes.DeleteMessageBatchRequestEntry, lambdas.MaxMessagesWithoutCheckpoint)
			intermediateReducedMap = make(aggregators.MapAggregator)
			r.Dedupe.WriteMap = lambdas.InitDedupeMap()
		}

		// call sqs receive messages
		output, err := r.QueuesAPI.ReceiveMessage(ctx, recieveMessageParams)
		if err != nil {
			reducerLogger.WithError(err).Error("Error reading from queue")
			return err
		}

		// process messages
		for _, message := range output.Messages {
			processedMessagesWithoutCheckpoint++

			// add delete info
			processedMessagesDeleteInfo[processedMessagesWithoutCheckpoint] = sqsTypes.DeleteMessageBatchRequestEntry{
				Id:            message.MessageId,
				ReceiptHandle: message.ReceiptHandle,
			}

			// get message attributes
			currentMapID := message.MessageAttributes[lambdas.MapIDAttribute].StringValue
			currentBatchID, err := strconv.Atoi(*message.MessageAttributes[lambdas.BatchIDAttribute].StringValue)
			if err != nil {
				reducerLogger.WithError(err).Error("Error getting message batch ID")
				return err
			}
			currentMessageID, err := strconv.Atoi(*message.MessageAttributes[lambdas.MessageIDAttribute].StringValue)
			if err != nil {
				reducerLogger.WithError(err).Error("Error getting message ID")
				return err
			}

			// check if message has already been processed
			if exists := r.Dedupe.BatchExists(*currentMapID, currentBatchID); exists {
				if r.Dedupe.IsBatchComplete(*currentMapID, currentBatchID) {
					// ignore as it is a duplicated message
					continue
				}

				if r.Dedupe.IsMessageProcessed(*currentMapID, currentBatchID, currentMessageID) {
					// ignore as it is a duplicated message
					continue
				}

				// message has not been processed
				// add processed message to dedupe map
				r.Dedupe.UpdateMessageProcessed(*currentMapID, currentBatchID, currentMessageID)

				// check if we are done processing batch from map
				if r.Dedupe.IsBatchComplete(*currentMapID, currentBatchID) {
					totalProcessedBatches++
					// delete processed map from dedupe
					r.Dedupe.DeletedProcessedMessages(*currentMapID, currentBatchID)
				}
			} else {
				// no messages for batch have been processed - init dedupe data for batch
				r.Dedupe.InitDedupeBatch(*currentMapID, currentBatchID, currentMessageID)
			}

			// process message
			// unmarshall message body
			var reduceMessage *aggregators.ReduceMessage
			body := []byte(*message.Body)
			err = json.Unmarshal(body, &reduceMessage)
			if err != nil {
				return err
			}

			if err := intermediateReducedMap.Reduce(reduceMessage); err != nil {
				reducerLogger.WithError(err).Error("Error processing message")
				return err
			}
		}

		// check if we are done processing values
		if totalProcessedBatches == *totalBatchesToProcess {
			break
		}
	}

	// wait in case reducers is saving checkpoint in the background
	wg.Wait()

	// update output map with reduced intermediate results
	wg.Add(1)
	go r.Output.UpdateOutput(intermediateReducedMap, &wg)

	// delete all messages from queue
	wg.Add(1)
	go r.DeleteIntermediateMessagesFromQueue(ctx, queueURL, processedMessagesDeleteInfo, &wg)

	wg.Wait()

	{{if .WithFilter}}
	// filter results
	r.Output = lambdas.RunFilter(r.Output, {{.PackageName}}.{{.FilterFunction}})
	{{end}}

	// generate key for output
	key := fmt.Sprintf("output/%s", r.ReducerID.String())
	
	{{if .WithSort}}
	// sort output
	sortedOutput := lambdas.RunSort(r.Output, {{.PackageName}}.{{.SortFunction}})

	// write sorted reducer output
	err = r.WriteSortedReducerOutput(ctx, sortedOutput, key)
	if err != nil {
		reducerLogger.WithError(err).Error("Error writing reducer output")
		return err
	}
	{{ else }}
	// write unsorted reducer output
	err = r.WriteReducerOutput(ctx, r.Output, key)
	if err != nil {
		reducerLogger.WithError(err).Error("Error writing reducer output")
		return err
	}
	{{end}}

	// indicate reducer has finished
	err = r.SendFinishedEvent(ctx)
	if err != nil {
		reducerLogger.WithError(err).Error("Error sending done message")
		return err
	}

	return nil
}

func main() {
	lambda.Start(HandleRequest)
}

`

const reduceMapFinalAggregatorTemplate = `
// Code generated by ribble DO NOT EDIT.
// |\   \\\\__     o
// | \_/    o \    o
// > _   (( <_  oo
// | / \__+___/
// |/     |/

package main

import (
	"context"
	"encoding/json"
	"fmt"
	"sync"

	"github.com/aws/aws-lambda-go/lambda"
	"github.com/aws/aws-sdk-go-v2/service/sqs"
	sqsTypes "github.com/aws/aws-sdk-go-v2/service/sqs/types"
	"github.com/josenarvaezp/displ/pkg/lambdas"
	"github.com/josenarvaezp/displ/pkg/aggregators"
	log "github.com/sirupsen/logrus"

	{{ if or .WithFilter .WithSort }} 
	"{{.PackagePath}}"
	{{ end }}
)

var r *lambdas.Reducer

func init() {
	// set logger
	log.SetLevel(log.ErrorLevel)

	var err error
	r, err = lambdas.NewRandomReducer({{.Local}})
	if err != nil {
		log.WithError(err).Fatal("Error starting reducer")
		return
	}
}

func HandleRequest(ctx context.Context, request lambdas.ReducerInput) error {
	// update reducer
	r.UpdateReducerWithRequest(ctx, request)

	// get reduce queue information
	queueName := fmt.Sprintf("%s-%s", r.JobID.String(), "final-aggregator")
	queueURL := lambdas.GetQueueURL(queueName, r.Region, r.AccountID, r.Local)

	// set reducer logger
	reducerLogger := log.WithFields(log.Fields{
		"Job ID":          r.JobID.String(),
		"Reducer ID":      r.ReducerID.String(),
		"Queue Partition": r.QueuePartition,
	})

	// set wait group
	var wg sync.WaitGroup

	// get checkpoint data
	checkpointData, err := r.GetCheckpointData(ctx, &wg)
	if err != nil {
		reducerLogger.WithError(err).Error("Error reading checkpoint")
		return err
	}
	// checkpoint info
	processedMessagesWithoutCheckpoint := 0
	checkpointData.LastCheckpoint++

	// number of messages the reducer needs to process - once per reducer
	totalMessagesToProcess, err := r.GetNumberOfMessagesToProcessFinalAggregator(ctx, request.NumReducers)
	if err != nil {
		reducerLogger.WithError(err).Error("Error getting queue metadata")
		return err
	}
	totalProcessedMessages := 0

	// processedMessagesDeleteInfo holds the data to delete messages from queue
	processedMessagesDeleteInfo := make([]sqsTypes.DeleteMessageBatchRequestEntry, lambdas.MaxMessagesWithoutCheckpoint)

	// holds the intermediate results
	intermediateOutput := make(aggregators.MapAggregator)

	// use same parameters for all get messages requests
	recieveMessageParams := &sqs.ReceiveMessageInput{
		QueueUrl:            &queueURL,
		MaxNumberOfMessages: lambdas.MaxItemsPerBatch,
		MessageAttributeNames: []string{
			lambdas.MessageIDAttribute,
		},
		WaitTimeSeconds: int32(5),
	}

	// recieve messages until we are done processing all queue
	for true {
		if processedMessagesWithoutCheckpoint == lambdas.MaxMessagesBeforeCheckpointComplete && checkpointData.LastCheckpoint != 1 {
			// check that the last checkpoint has completed before processing any more messages
			// we give a buffer of 15,000 new messages for saving the checkpoint which happens
			// in the background. If this point is reached it means we have processed 115,000 messages
			// without deleting from the queue which is close to the aws limit for queues
			wg.Wait()
		}

		if processedMessagesWithoutCheckpoint == lambdas.MaxMessagesWithoutCheckpoint {
			// We need to delete the messages read from the sqs queue and we create a checkpoint
			// in S3 as the fault tolerant mechanism. Saving the checkpoint can be done concurrently
			// in the background while we keep processing messages

			// merge the dedupe map so that the read dedupe map is up to date
			r.DedupeSimple.Merge()

			// save intermediate dedupe
			wg.Add(1)
			go r.SaveIntermediateDedupe(ctx, checkpointData.LastCheckpoint, r.DedupeSimple.ReadMap, &wg)

			// save intermediate map
			wg.Add(1)
			go r.SaveIntermediateOutput(ctx, intermediateOutput, checkpointData.LastCheckpoint, &wg)

			// update output map with reduced intermediate results
			wg.Add(1)
			go r.Output.UpdateOutput(intermediateOutput, &wg)

			// delete all messages from queue
			wg.Add(1)
			go r.DeleteIntermediateMessagesFromQueue(ctx, queueURL, processedMessagesDeleteInfo, &wg)

			// update checkpoint info
			checkpointData.LastCheckpoint++
			processedMessagesWithoutCheckpoint = 0
			processedMessagesDeleteInfo = make([]sqsTypes.DeleteMessageBatchRequestEntry, lambdas.MaxMessagesWithoutCheckpoint)
			intermediateOutput = make(aggregators.MapAggregator)
			r.DedupeSimple.WriteMap = lambdas.InitDedupeSimpleMap()
		}

		// call sqs receive messages
		output, err := r.QueuesAPI.ReceiveMessage(ctx, recieveMessageParams)
		if err != nil {
			reducerLogger.WithError(err).Error("Error reading from queue")
			return err
		}

		// process messages
		for _, message := range output.Messages {
			processedMessagesWithoutCheckpoint++

			// add delete info
			processedMessagesDeleteInfo[processedMessagesWithoutCheckpoint] = sqsTypes.DeleteMessageBatchRequestEntry{
				Id:            message.MessageId,
				ReceiptHandle: message.ReceiptHandle,
			}

			// get message attributes
			currentMessageID := *message.MessageAttributes[lambdas.MessageIDAttribute].StringValue

			// check if message has already been processed
			if !r.DedupeSimple.IsMessageProcessed(currentMessageID) {

				// process message
				// unmarshall message body
				var reduceMessage *aggregators.ReduceMessage
				body := []byte(*message.Body)
				err = json.Unmarshal(body, &reduceMessage)
				if err != nil {
					return err
				}

				// process message
				if err := intermediateOutput.Reduce(reduceMessage); err != nil {
					reducerLogger.WithError(err).Error("Error processing message")
					return err
				}

				// update dedupe and messages processed count
				r.DedupeSimple.UpdateMessageProcessed(currentMessageID)
				totalProcessedMessages++
			}
		}

		// check if we are done processing values
		if totalProcessedMessages == *totalMessagesToProcess {
			break
		}
	}

	// wait in case reducers is saving checkpoint in the background
	wg.Wait()

	// update output map with reduced intermediate results
	wg.Add(1)
	go r.Output.UpdateOutput(intermediateOutput, &wg)

	// delete all messages from queue
	wg.Add(1)
	go r.DeleteIntermediateMessagesFromQueue(ctx, queueURL, processedMessagesDeleteInfo, &wg)

	wg.Wait()

	{{if .WithFilter}}
	// filter results
	r.Output = lambdas.RunFilter(r.Output, {{.PackageName}}.{{.FilterFunction}})
	{{end}}

	// generate key for output
	key := "output"
	
	{{if .WithSort}}
	// sort output
	sortedOutput := lambdas.RunSort(r.Output, {{.PackageName}}.{{.SortFunction}})

	// write sorted reducer output
	err = r.WriteSortedReducerOutput(ctx, sortedOutput, key)
	if err != nil {
		reducerLogger.WithError(err).Error("Error writing reducer output")
		return err
	}
	{{ else }}
	// write unsorted reducer output
	err = r.WriteReducerOutput(ctx, r.Output, key)
	if err != nil {
		reducerLogger.WithError(err).Error("Error writing reducer output")
		return err
	}
	{{end}}

	return nil
}

func main() {
	lambda.Start(HandleRequest)
}

`
